
-- Create Datagen Connector job / creating transactions -> Source connector
curl --request POST \
  --url 'http://127.0.0.1:8083/connectors' \
    --header 'content-type: application/json' \
  --data '{
  "name": "DatagenConnectorConnector_0",
  "config": {
    "name": "DatagenConnectorConnector_0",
    "connector.class": "io.confluent.kafka.connect.datagen.DatagenConnector",
    "kafka.topic": "transactions",
    "quickstart": "TRANSACTIONS"
  }
}'

-- List Connectors
curl --request GET \
  --url 'http://127.0.0.1:8083/connectors' \ |jq
  
-- List a specific connector status
curl --request GET \
  --url 'http://127.0.0.1:8083/connectors/DatagenConnectorConnector_0/status' \
   | jq '.'
   

-- list plugins installed
curl --request GET \
 --url 'http://127.0.0.1:8083/connector-plugins' \
 | jq '.'
  
-- if CP stack is the tar.gz file based
confluent local services connect plugin list |grep "class"


-- create sink into local containerised mysql instance.


-- https://stackoverflow.com/questions/51758456/no-suitable-driver-found-for-jdbcmysql-in-kafka-connect/51760565#51760565
-- MySQL Sink connector
curl --request POST \
  --url 'http://127.0.0.1:8083/connectors' \
  --header 'content-type: application/json' \
  --data '{
  "name": "MySqlSinkConnector_0",
  "config": {
  	"connector.class": "io.confluent.connect.jdbc.JdbcSinkConnector",
  	"name": "MySqlSinkConnector_0",
  	"topics": "transactions",
 	"connection.url": "jdbc:mysql://172.16.20.29:3306/demo",
  	"input.data.format": "AVRO",
  	"input.key.format": "AVRO",
  	"connection.user": "kc101user",
  	"connection.password": "kc101pw",
  	"db.name": "demo",
  	"insert.mode": "INSERT",
  	"auto.create": "true",
  	"auto.evolve": "true",
  	"tasks.max": "1"
	}
}'


Blog https://rmoff.net/2020/06/17/loading-csv-data-into-kafka/
Code https://github.com/confluentinc/demo-scene/tree/master/csv-to-kafka

-- straight load from csv, generating schema
curl -i -X PUT -H "Accept:application/json" \
     -H  "Content-Type:application/json" http://localhost:8083/connectors/source-csv-spooldir-00/config \
     -d '{
         "connector.class": "com.github.jcustenborder.kafka.connect.spooldir.SpoolDirCsvSourceConnector",
         "topic": "orders_spooldir_00",
         "input.path": "/data/unprocessed",
         "finished.path": "/data/processed",
         "error.path": "/data/error",
         "input.file.pattern": ".*\\.csv",
         "schema.generation.enabled":"true",
         "csv.first.row.as.header":"true"
         }'


-- adding a key field.
curl -i -X PUT -H "Accept:application/json" \
    -H  "Content-Type:application/json" http://localhost:8083/connectors/source-csv-spooldir-01/config \
    -d '{
        "connector.class": "com.github.jcustenborder.kafka.connect.spooldir.SpoolDirCsvSourceConnector",
        "topic": "orders_spooldir_01",
        "input.path": "/data/unprocessed",
        "finished.path": "/data/processed",
        "error.path": "/data/error",
        "input.file.pattern": ".*\\.csv",
        "schema.generation.enabled":"true",
        "schema.generation.key.fields":"order_id",
        "csv.first.row.as.header":"true"
        }'


--  changing field types
curl -i -X PUT -H "Accept:application/json" \
    -H  "Content-Type:application/json" http://localhost:8083/connectors/source-csv-spooldir-02/config \
    -d '{
        "connector.class": "com.github.jcustenborder.kafka.connect.spooldir.SpoolDirCsvSourceConnector",
        "topic": "orders_spooldir_02",
        "input.path": "/data/unprocessed",
        "finished.path": "/data/processed",
        "error.path": "/data/error",
        "input.file.pattern": ".*\\.csv",
        "schema.generation.enabled":"true",
        "schema.generation.key.fields":"order_id",
        "csv.first.row.as.header":"true",
        "transforms":"castTypes",
        "transforms.castTypes.type":"org.apache.kafka.connect.transforms.Cast$Value",
        "transforms.castTypes.spec":"order_id:int32,customer_id:int32,order_total_usd:float32"
        }'



-- Get/Inspect schema definition
 curl --silent --location --request GET 'http://localhost:8081/subjects/orders_spooldir_02-value/versions/latest' |jq '.schema|fromjson'
{
  "type": "record", "name": "Value", "namespace": "com.github.jcustenborder.kafka.connect.model",
  "fields": [
    { "name": "order_id", "type": [ "null", "int" ], "default": null },
    { "name": "customer_id", "type": [ "null", "int" ], "default": null },
    { "name": "order_total_usd", "type": [ "null", "float" ], "default": null },
    { "name": "make", "type": [ "null", "string" ], "default": null },
    { "name": "model", "type": [ "null", "string" ], "default": null },
    { "name": "delivery_city", "type": [ "null", "string" ], "default": null },
    { "name": "delivery_company", "type": [ "null", "string" ], "default": null },
    { "name": "delivery_address", "type": [ "null", "string" ], "default": null }
  ],
  "connect.name": "com.github.jcustenborder.kafka.connect.model.Value"
}


-- straight load of csv into a simple string, no schema
curl -i -X PUT -H "Accept:application/json" \
    -H  "Content-Type:application/json" http://localhost:8083/connectors/source-csv-spooldir-03/config \
    -d '{
        "connector.class": "com.github.jcustenborder.kafka.connect.spooldir.SpoolDirLineDelimitedSourceConnector",
        "value.converter":"org.apache.kafka.connect.storage.StringConverter",
        "topic": "orders_spooldir_03",
        "input.path": "/data/unprocessed",
        "finished.path": "/data/processed",
        "error.path": "/data/error",
        "input.file.pattern": ".*\\.csv"
        }'


-- Sink into Postgresql

-- primary key based on key assigned for orders_spooldir_01 step.
curl -X PUT http://localhost:8083/connectors/sink-postgres-orders-00/config \
    -H "Content-Type: application/json" \
    -d '{
        "connector.class": "io.confluent.connect.jdbc.JdbcSinkConnector",
        "connection.url": "jdbc:postgresql://mbp-local:5432/",
        "connection.user": "postgres",
        "connection.password": "password",
        "tasks.max": "1",
        "topics": "orders_spooldir_02",
        "auto.create": "true",
        "auto.evolve":"true",
        "pk.mode":"record_value",
        "pk.fields":"order_id",
        "insert.mode": "upsert",
        "table.name.format":"orders"
    }'

-- ksql
-- Access ksqldb cli
docker exec -it ksqldb-cli ksql http://mbp.local:8088

CREATE STREAM ORDERS_02 WITH (KAFKA_TOPIC='orders_spooldir_02', VALUE_FORMAT='AVRO');
DESCRIBE ORDERS_02;
SELECT DELIVERY_CITY, COUNT(*) AS ORDER_COUNT, MAX(CAST(ORDER_TOTAL_USD AS DECIMAL(9,2))) AS BIGGEST_ORDER_USD FROM ORDERS_02 GROUP BY DELIVERY_CITY EMIT CHANGES;


-- start kcat pod manually, this has been replaced with a container as part of the docker-compose.yaml
docker run -d \
  --network aws-kafka-confluent_default \
  --name kafkacat \
  --entrypoint bash -i \
  confluentinc/cp-kcat:latest
  
  
-- now added to docker-compose.yaml file
docker exec -t kafkacat  kcat \
  -b broker:29092 \
  -t orders_spooldir_02 \
  -C -o-1 -J \
  -s key=s -s value=avro \
  -r http://schema-registry:8081 \
  | jq '.payload'
